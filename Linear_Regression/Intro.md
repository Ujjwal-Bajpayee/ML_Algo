# LINEAR REGRESSION - INTERVIEW GUIDE

## Table of Contents
1. [What is Linear Regression?](#1-what-is-linear-regression)
2. [Mathematical Foundation](#2-mathematical-foundation)
3. [Types of Linear Regression](#3-types-of-linear-regression)
4. [Real-World Examples](#4-real-world-worked-example)
5. [Assumptions](#5-assumptions-of-linear-regression)
6. [Implementation (Python)](#6-implementation-python)
7. [Common Interview Questions](#7-common-interview-questions--answers)
8. [Quick Cheat Sheet](#8-quick-cheat-sheet)

---

## 1. What is Linear Regression?

Linear Regression is a **supervised machine learning algorithm** that models the relationship between:
- **Independent variable(s) [X]** - Features/Predictors
- **Dependent variable [y]** - Target/Response

### Goal
Find the best-fit line (or hyperplane) that minimizes the error between predicted and actual values.

### Intuition
Imagine plotting points on a graph and drawing a straight line that passes as close as possible to all points. That's linear regression!

---

## 2. Mathematical Foundation

### Equation (Simple Linear Regression)
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx
```

Where:
- `≈∑` = Predicted value
- `Œ≤‚ÇÄ` = Intercept (bias term)
- `Œ≤‚ÇÅ` = Slope (coefficient)
- `x` = Input feature

### Equation (Multiple Linear Regression)
```
≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô
```

Or in matrix form:
```
≈∑ = XŒ≤
```

Where:
- `X` = Feature matrix (n_samples √ó n_features)
- `Œ≤` = Coefficient vector
- `≈∑` = Predictions

### Cost Function (Mean Squared Error)
```
MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

**Goal:** Minimize MSE to find optimal Œ≤‚ÇÄ and Œ≤‚ÇÅ

### Closed-Form Solution (Normal Equation)
```
Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
```

### Gradient Descent Update Rule
```
Œ≤ := Œ≤ - Œ± ¬∑ ‚àÇ(Cost)/‚àÇŒ≤
```

Where `Œ±` is the learning rate

---

## 3. Types of Linear Regression

### A. Simple Linear Regression

**Definition:** One independent variable predicts one dependent variable

**Equation:** `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ`

#### Example 1: Predicting House Price based on Size
- X = House size (sq ft)
- y = House price ($)
- **Model:** `Price = 50,000 + 100 √ó Size`

#### Example 2: Predicting Salary based on Years of Experience
- X = Years of experience
- y = Salary
- **Model:** `Salary = 30,000 + 5,000 √ó Experience`

---

### B. Multiple Linear Regression

**Definition:** Multiple independent variables predict one dependent variable

**Equation:** `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ`

#### Example 1: Predicting House Price
- X‚ÇÅ = Size (sq ft)
- X‚ÇÇ = Number of bedrooms
- X‚ÇÉ = Age of house
- X‚ÇÑ = Distance from city center
- **Model:** `Price = 50,000 + 100√óSize + 15,000√óBedrooms - 2,000√óAge - 500√óDistance`

#### Example 2: Predicting Student Performance
- X‚ÇÅ = Study hours
- X‚ÇÇ = Previous test score
- X‚ÇÉ = Attendance %
- **Model:** `Score = 10 + 5√óStudyHours + 0.4√óPrevScore + 0.3√óAttendance`

---

### C. Polynomial Regression (Extension)

**Definition:** Models non-linear relationships using polynomial features

**Equation:** `y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø`

#### Example: Predicting crop yield based on fertilizer amount
- Too little fertilizer ‚Üí low yield
- Optimal amount ‚Üí maximum yield
- Too much fertilizer ‚Üí yield decreases (parabolic relationship)

---

## 4. Real-World Worked Example

### Problem: Predict salary based on years of experience

#### Data
| Experience (X) | Salary (y) |
|----------------|------------|
| 1              | 40,000     |
| 2              | 45,000     |
| 3              | 50,000     |
| 4              | 55,000     |
| 5              | 60,000     |

#### Step 1: Calculate means
```
XÃÑ = (1+2+3+4+5)/5 = 3
»≥ = (40+45+50+55+60)/5 = 50 (in thousands)
```

#### Step 2: Calculate slope (Œ≤‚ÇÅ)
```
Œ≤‚ÇÅ = Œ£[(x·µ¢ - XÃÑ)(y·µ¢ - »≥)] / Œ£[(x·µ¢ - XÃÑ)¬≤]
   = [(-2)(-10) + (-1)(-5) + (0)(0) + (1)(5) + (2)(10)] / [4 + 1 + 0 + 1 + 4]
   = [20 + 5 + 0 + 5 + 20] / 10
   = 50 / 10
   = 5
```

#### Step 3: Calculate intercept (Œ≤‚ÇÄ)
```
Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅXÃÑ
   = 50 - 5(3)
   = 35
```

### Final Model
```
Salary = 35,000 + 5,000 √ó Experience
```

### Interpretation
- Base salary (0 years experience): **$35,000**
- For each additional year of experience, salary increases by **$5,000**

### Prediction
For 6 years of experience:
```
Salary = 35,000 + 5,000(6) = $65,000
```

---

## 5. Assumptions of Linear Regression

### 1. Linearity
- Relationship between X and y is linear
- **Check:** Residual plots should show random scatter

### 2. Independence
- Observations are independent of each other
- **Check:** No autocorrelation (Durbin-Watson test)

### 3. Homoscedasticity
- Constant variance of residuals
- **Check:** Residuals vs fitted values plot (no funnel shape)

### 4. Normality
- Residuals are normally distributed
- **Check:** Q-Q plot, Shapiro-Wilk test

### 5. No Multicollinearity (for Multiple Regression)
- Independent variables are not highly correlated
- **Check:** VIF (Variance Inflation Factor) < 10

---

## 6. Implementation (Python)

### Method 1: Using scikit-learn (Recommended)

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Experience (years)
y = np.array([40, 45, 50, 55, 60])            # Salary (in thousands)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_}")
print(f"Coefficient (Œ≤‚ÇÅ): {model.coef_[0]}")
print(f"R¬≤ Score: {r2_score(y_test, y_pred)}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}")

# Predict new value
new_experience = np.array([[6]])
predicted_salary = model.predict(new_experience)
print(f"Predicted salary for 6 years: ${predicted_salary[0]*1000}")
```

---

### Method 2: From Scratch (Using Normal Equation)

```python
import numpy as np

class LinearRegressionFromScratch:
    def __init__(self):
        self.coefficients = None
        self.intercept = None
    
    def fit(self, X, y):
        # Add bias term (column of 1s)
        X_bias = np.c_[np.ones((X.shape[0], 1)), X]
        
        # Normal equation: Œ≤ = (X^T X)^(-1) X^T y
        theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y
        
        self.intercept = theta[0]
        self.coefficients = theta[1:]
    
    def predict(self, X):
        return self.intercept + X @ self.coefficients

# Usage
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([40, 45, 50, 55, 60])

model = LinearRegressionFromScratch()
model.fit(X, y)

print(f"Intercept: {model.intercept}")
print(f"Coefficients: {model.coefficients}")
print(f"Prediction for X=6: {model.predict(np.array([[6]]))}")
```

---

### Method 3: Using Gradient Descent

```python
import numpy as np

def gradient_descent_linear_regression(X, y, learning_rate=0.01, epochs=1000):
    m = len(y)
    X_bias = np.c_[np.ones((m, 1)), X]  # Add bias term
    theta = np.zeros(X_bias.shape[1])   # Initialize parameters
    
    for epoch in range(epochs):
        # Predictions
        predictions = X_bias @ theta
        
        # Compute gradients
        errors = predictions - y
        gradients = (1/m) * X_bias.T @ errors
        
        # Update parameters
        theta -= learning_rate * gradients
        
        # Optional: Print cost every 100 epochs
        if epoch % 100 == 0:
            cost = (1/(2*m)) * np.sum(errors**2)
            print(f"Epoch {epoch}, Cost: {cost:.4f}")
    
    return theta

# Usage
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([40, 45, 50, 55, 60])

theta = gradient_descent_linear_regression(X, y, learning_rate=0.01, epochs=1000)
print(f"\nIntercept (Œ≤‚ÇÄ): {theta[0]}")
print(f"Coefficient (Œ≤‚ÇÅ): {theta[1]}")
```

---

## 7. Common Interview Questions & Answers

### Q1: What is the difference between Linear Regression and Logistic Regression?

**A:** Linear Regression predicts continuous values (e.g., house prices), while Logistic Regression predicts probabilities for classification (0 or 1). Linear uses MSE as loss; Logistic uses cross-entropy loss.

---

### Q2: How do you handle outliers in Linear Regression?

**A:** Methods include:
- Remove outliers (if data errors)
- Transform features (log transformation)
- Use robust regression techniques (Huber regression, RANSAC)
- Regularization (Ridge, Lasso)

---

### Q3: What is R¬≤ (R-squared) and how do you interpret it?

**A:** R¬≤ measures the proportion of variance in y explained by X.

- **Range:** 0 to 1 (can be negative for bad models)
- **Example:** R¬≤ = 0.8 means 80% of variance is explained by the model
- **Higher R¬≤ = better fit**
- **Formula:** `R¬≤ = 1 - (SS_res / SS_tot)`

---

### Q4: What is the difference between Ridge and Lasso regression?

**A:** Both add regularization to prevent overfitting:

**Ridge (L2):** Adds penalty = `Œª Œ£Œ≤¬≤`
- Shrinks coefficients but doesn't set them to zero
- Use when all features are relevant

**Lasso (L1):** Adds penalty = `Œª Œ£|Œ≤|`
- Can set coefficients to exactly zero (feature selection)
- Use for sparse models with many irrelevant features

**ElasticNet:** Combines both L1 and L2 penalties

---

### Q5: When should you NOT use Linear Regression?

**A:** Avoid when:
- Relationship is highly non-linear (use polynomial/tree-based models)
- Target variable is categorical (use classification algorithms)
- Assumptions are severely violated
- Severe multicollinearity exists
- Small dataset with many features (risk of overfitting)

---

### Q6: How do you choose between Gradient Descent and Normal Equation?

**A:** 

**Normal Equation:**
- ‚úÖ Pro: No need to choose learning rate, gives exact solution
- ‚ùå Con: Slow for large features (O(n¬≥) complexity), needs invertible X·µÄX
- üìå Use when: n < 10,000 features

**Gradient Descent:**
- ‚úÖ Pro: Works well with large datasets, scales better
- ‚ùå Con: Need to tune learning rate, requires more iterations
- üìå Use when: n > 10,000 features or online learning

---

### Q7: What is multicollinearity and how does it affect your model?

**A:** Multicollinearity occurs when independent variables are highly correlated.

**Effects:**
- Unstable coefficient estimates
- High variance in predictions
- Difficult to interpret individual feature importance

**Detection:**
- Correlation matrix (|r| > 0.8)
- VIF > 10

**Solutions:**
- Remove correlated features
- Use PCA
- Ridge regression

---

### Q8: How do you evaluate a Linear Regression model?

**A:** 

**Metrics:**
- **R¬≤ (R-squared):** Proportion of variance explained (0 to 1)
- **Adjusted R¬≤:** R¬≤ adjusted for number of predictors
- **MSE (Mean Squared Error):** Average squared error
- **RMSE (Root Mean Squared Error):** Square root of MSE (same unit as y)
- **MAE (Mean Absolute Error):** Average absolute error

**Visual checks:**
- Residual plots (should be random)
- Q-Q plot (check normality)
- Actual vs Predicted plot

---

### Q9: What is the bias-variance tradeoff in Linear Regression?

**A:** 

**Bias:** Error from incorrect assumptions (underfitting)
- Simple models ‚Üí high bias ‚Üí systematic error

**Variance:** Error from sensitivity to training data (overfitting)
- Complex models ‚Üí high variance ‚Üí unstable predictions

**Goal:** Find balance
- Use regularization (Ridge/Lasso) to reduce variance
- Add features or polynomial terms to reduce bias

---

### Q10: How do you handle categorical variables in Linear Regression?

**A:** Use encoding techniques:

**One-Hot Encoding (for nominal variables):**
- Color = {Red, Blue, Green}
- Create: Color_Red, Color_Blue, Color_Green (0 or 1)
- Drop one column to avoid multicollinearity (dummy variable trap)

**Label Encoding (for ordinal variables):**
- Education = {High School, Bachelor, Master, PhD}
- Encode as: {0, 1, 2, 3}

---

## 8. Quick Cheat Sheet

### Key Equations

| Equation | Formula |
|----------|---------|
| Simple Linear Regression | `≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx` |
| Multiple Linear Regression | `≈∑ = Œ≤‚ÇÄ + Œ£(Œ≤·µ¢x·µ¢)` |
| Slope (Œ≤‚ÇÅ) | `Œ≤‚ÇÅ = Œ£[(x·µ¢-xÃÑ)(y·µ¢-»≥)] / Œ£(x·µ¢-xÃÑ)¬≤` |
| Intercept (Œ≤‚ÇÄ) | `Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ` |
| Normal Equation | `Œ≤ = (X·µÄX)‚Åª¬πX·µÄy` |
| Cost Function (MSE) | `J(Œ≤) = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤` |
| R-squared | `R¬≤ = 1 - (SS_res / SS_tot)` |
| Adjusted R¬≤ | `R¬≤_adj = 1 - [(1-R¬≤)(n-1)/(n-k-1)]` |

---

### Evaluation Metrics

| Metric | Formula | Description |
|--------|---------|-------------|
| MSE | `(1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤` | Lower is better, penalizes large errors |
| RMSE | `‚àöMSE` | Same unit as target variable |
| MAE | `(1/n) Œ£\|y·µ¢ - ≈∑·µ¢\|` | Less sensitive to outliers |
| R¬≤ | `1 - SS_res/SS_tot` | 0 to 1, higher is better |

---

### Regularization

| Type | Formula | Use Case |
|------|---------|----------|
| Ridge (L2) | `Cost = MSE + Œª Œ£Œ≤·µ¢¬≤` | Shrinks coefficients |
| Lasso (L1) | `Cost = MSE + Œª Œ£\|Œ≤·µ¢\|` | Feature selection |
| ElasticNet | `Cost = MSE + Œª‚ÇÅŒ£\|Œ≤·µ¢\| + Œª‚ÇÇŒ£Œ≤·µ¢¬≤` | Combination |

---

### Assumptions Checklist

- ‚úÖ **Linearity** ‚Üí Residual plot
- ‚úÖ **Independence** ‚Üí Durbin-Watson test
- ‚úÖ **Homoscedasticity** ‚Üí Scale-location plot
- ‚úÖ **Normality of residuals** ‚Üí Q-Q plot
- ‚úÖ **No multicollinearity** ‚Üí VIF < 10

---

### When to Use

**‚úÖ Use Linear Regression when:**
- Predicting continuous values
- Linear relationships
- Interpretable models needed
- Baseline model

**‚ùå Don't use when:**
- Classification problems
- Highly non-linear relationships
- Complex interactions
- Large number of features (without regularization)

---

### Python Quick Reference

```python
# Train model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Get parameters
model.coef_        # Coefficients
model.intercept_   # Intercept

# Predict
y_pred = model.predict(X_test)

# Evaluate
from sklearn.metrics import r2_score, mean_squared_error
r2_score(y_test, y_pred)
mean_squared_error(y_test, y_pred)
```

---

## Tips for Interview Success

1. ‚úÖ Always mention assumptions when discussing Linear Regression
2. ‚úÖ Be ready to explain the difference between Simple and Multiple LR
3. ‚úÖ Understand when to use Normal Equation vs Gradient Descent
4. ‚úÖ Know how to handle categorical variables and outliers
5. ‚úÖ Be familiar with regularization techniques (Ridge, Lasso)
6. ‚úÖ Explain evaluation metrics clearly (R¬≤, RMSE, MAE)
7. ‚úÖ Practice coding Linear Regression from scratch
8. ‚úÖ Understand bias-variance tradeoff
9. ‚úÖ Know the limitations of Linear Regression
10. ‚úÖ Be ready with real-world examples from your domain


